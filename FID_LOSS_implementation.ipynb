{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN5DjjreVIE1bQNl8nXbqiv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Stanfording/Trying_FID_LOSS/blob/main/FID_LOSS_implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# Goal:   Testing if FID Loss works on training GAN\n",
        "\n"
      ],
      "metadata": {
        "id": "7gaYl2YwkRiu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get the preprocessed celebHD data from google drive"
      ],
      "metadata": {
        "id": "piBy5s1LBUw_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #Download the dataset\n",
        "# !wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1KqBRLsB0CJuQGycvaPINwaPgcGDUsAxN' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1KqBRLsB0CJuQGycvaPINwaPgcGDUsAxN\" -O \"proCeleba.zip\" && rm -rf /tmp/cookies.txt\n",
        "\n",
        "# #unzip the dataset\n",
        "# !unzip \"/content/proCeleba.zip\"\n",
        "\n",
        "# #remove unnecessary files\n",
        "# !rm -rf /content/__MACOSX"
      ],
      "metadata": {
        "id": "Zp1iaK-uBRrV"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import libraries"
      ],
      "metadata": {
        "id": "03mgtLjDBfOy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "from torch.autograd import Variable, grad\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "import torch as t\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets, transforms, utils\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from skimage import io\n",
        "import copy"
      ],
      "metadata": {
        "id": "s_TyTXFBBhS1"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set initial hyperparameters"
      ],
      "metadata": {
        "id": "QwBlsx3WBmOd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "\n",
        "resolution = 16\n",
        "\n",
        "img_fold_dir_64_reso = f\"/content/proCeleba/{resolution}\"\n",
        "\n",
        "iteration = 200\n",
        "\n",
        "critic = 5          \n",
        "\n",
        "eval_size = 25\n",
        "\n",
        "laten_space = 100\n",
        "\n",
        "device = t.device('cuda' if t.cuda.is_available() else 'cpu')\n",
        "\n",
        "log_folder = \"log\"\n",
        "!mkdir \"log\"\n",
        "!mkdir \"log/checkpoint\"\n",
        "!mkdir \"log/sample\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jk8t27hRBnqr",
        "outputId": "788b9e20-3d06-44b9-b6c9-84a5b0c6d811"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘log’: File exists\n",
            "mkdir: cannot create directory ‘log/checkpoint’: File exists\n",
            "mkdir: cannot create directory ‘log/sample’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Show me which gpu I am using."
      ],
      "metadata": {
        "id": "mPrOMKEbBylH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi -L"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gi64DRdjBzd4",
        "outputId": "f1c7a77c-4e2c-4d38-bc21-1485843341b1"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0: Tesla P100-PCIE-16GB (UUID: GPU-d82b2910-d822-9127-b0ca-e11afed897e5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pre loading the data"
      ],
      "metadata": {
        "id": "koic4if7B1MT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a data class for load unclassfied data.\n",
        "class Get_No_Classes_Img_Dataset(Dataset):\n",
        "    \n",
        "    def __init__(self, folder_dir, transform = None):\n",
        "        self.folder_dir = os.path.join(folder_dir)\n",
        "        self.transform = transform\n",
        "        self.image_list = os.listdir(self.folder_dir)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(os.listdir(self.folder_dir))\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        \n",
        "        image_name = self.image_list[index]\n",
        "        \n",
        "        image_dir = os.path.join(self.folder_dir, image_name)\n",
        "        \n",
        "        image = io.imread(image_dir)\n",
        "        \n",
        "        if (self.transform != None):\n",
        "            image = self.transform(image)\n",
        "        \n",
        "        return image\n",
        "    \n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor()       #From Batch * Highth * Width * Channel to Batch * Channel * Highth * Width\n",
        "                                  #Which is what pytorch CNN can work with.\n",
        "]) \n",
        "\n",
        "dataset = Get_No_Classes_Img_Dataset(img_fold_dir_64_reso, transform = transform) \n",
        "                                                            # datasets[0].shape = (16,16,3)\n",
        "                                                            # len(datasets) = 28000\n",
        "total_data_len = len(dataset)\n",
        "                                                                                            \n",
        "#datasets_batched = DataLoader(datasets, batch_size = batch_size) #loader is renewed every epoch\n"
      ],
      "metadata": {
        "id": "Vt4rxDBeB6fo"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "loader = iter(loader)\n",
        "print(next(loader).shape) \n",
        "```\n",
        "will output\n",
        "\n",
        "\n",
        "```\n",
        "torch.Size([batch_size, 3, resolution, resolution])\n",
        "```\n",
        "\n",
        "So data loading is ready.\n",
        "\n",
        "What's left is keep using \n",
        "\n",
        "```\n",
        "next(loader)\n",
        "```\n",
        "to access each batch of data\n"
      ],
      "metadata": {
        "id": "J8EFVPstlDHD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize a picture"
      ],
      "metadata": {
        "id": "J2dOjqrRCYOR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Viewing one data sample function:\n",
        "def showOneImge(img, i, shouldSave):\n",
        "    \n",
        "    img = img.squeeze()\n",
        "    \n",
        "    img = transforms.ToPILImage()(img)\n",
        "    \n",
        "    plt.figure(figsize = (10,10), dpi = 10)\n",
        "    plt.axis('off')\n",
        "    \n",
        "    if shouldSave:\n",
        "      saveDir = f'{log_folder}/sample/{str(i).zfill(6)}.png'\n",
        "      plt.imshow(img)\n",
        "      plt.savefig(saveDir, bbox_inches='tight', pad_inches = 0)\n",
        "      img = Image(saveDir)\n",
        "      display(img)\n",
        "    else:\n",
        "      deleteDir = f\"{log_folder}/sample/Delete.png\"\n",
        "      plt.imshow(img)\n",
        "      plt.savefig(deleteDir, bbox_inches='tight', pad_inches = 0)\n",
        "      img = Image(deleteDir)\n",
        "      display(img)\n",
        "      !rm \"/content/log/sample/Delete.png\"\n",
        "    return \n",
        "\n",
        "''' Testing showOneImage'''\n",
        "# loader = DataLoader(datasets, batch_size = batch_size)\n",
        "\n",
        "# data = iter(loader)\n",
        "\n",
        "# oneSample = next(data)[0]\n",
        "\n",
        "# showOneImge(oneSample, 9999, True)\n",
        "\n",
        "\n",
        "def showMoreImages(img, num):\n",
        "\n",
        "  subplot_x = int(num ** (1/2))\n",
        "  subplot_y = num // subplot_x\n",
        "  plt.figure(figsize = (2,2))\n",
        "  for i in range(len(img)):\n",
        "      aimg = transforms.ToPILImage()(img[i])\n",
        "      plt.subplot(subplot_x, subplot_y, i+1)\n",
        "      plt.imshow(aimg)\n",
        "      plt.axis('off')\n"
      ],
      "metadata": {
        "id": "EjQccnhTCZgU"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "-------------------------------------------------\n",
        "\n",
        "### Now Designing the simple GAN network"
      ],
      "metadata": {
        "id": "xf_Ocw2onofc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Define the generator\n",
        "\"\"\"\n",
        "\n",
        "class G(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.laten = nn.Sequential(\n",
        "            nn.Linear(laten_space, 100),\n",
        "            nn.Linear(100, 500),\n",
        "            nn.Linear(500, 128 * resolution * resolution))\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(128, 64, (3, 3), padding = \"same\"),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.Conv2d(64, 32, (3, 3), padding = \"same\"),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout2d(),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.Conv2d(32, 3, (3, 3), padding = \"same\"),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        \n",
        "        \n",
        "        \n",
        "    def forward(self, theInput, batch_size):\n",
        "        \n",
        "        x = self.laten(theInput)\n",
        "        \n",
        "        x = t.reshape(x, (batch_size, 128, resolution, resolution))\n",
        "         \n",
        "        x = self.model(x)\n",
        "            \n",
        "            \n",
        "        return x\n",
        "    \n",
        "class D(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.netWork = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, (3, 3), padding = \"same\"),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Dropout2d(0.3),\n",
        "            nn.Conv2d(64, 128, (3, 3), stride = (3, 3)),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(128, 1, (3, 3), stride = (3, 3)),\n",
        "            nn.Flatten())\n",
        "        \n",
        "    def forward(self, theInput):\n",
        "        \n",
        "        return self.netWork(theInput)"
      ],
      "metadata": {
        "id": "lhZEIrxxCjhv"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing the network"
      ],
      "metadata": {
        "id": "VVyrduFTClyR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "Testing the net work:\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# oneImg = next(iter(datasets))[0]\n",
        "\n",
        "# showOneImge(oneImg, 0)\n",
        "\n",
        "# oneImg = oneImg.expand(1,3,64,64)\n",
        "\n",
        "# print(oneImg.shape)\n",
        "\n",
        "# print(oneImg)\n",
        "\n",
        "# #img into G to test shape\n",
        "\n",
        "# input_noise_example = t.randn((batch_size, 1, 1, 5))\n",
        "\n",
        "# a = G()(input_noise_example, batch_size)\n",
        "# print(a[0])\n",
        "# showOneImge(a[0], 0)\n",
        "\n",
        "# b = D()(next(loader))\n",
        "\n",
        "# print(b.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "VnPyQQNbCpB0",
        "outputId": "d709ce8a-c38a-47e5-a25b-27247d308dfe"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nTesting the net work:\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gradient Penalty from wGAN."
      ],
      "metadata": {
        "id": "LiBvvKgwCrnL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_gradient_penalty(D, real_samples, fake_samples, current_batch_size):\n",
        "    \"\"\"Calculates the gradient penalty loss for WGAN GP\"\"\"\n",
        "    # Random weight term for interpolation between real and fake samples\n",
        "    alpha = t.randn((current_batch_size, 1, 1, 1)).to(device)\n",
        "    # Get random interpolation between real and fake samples\n",
        "    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
        "    \n",
        "    \n",
        "    d_interpolates = D(interpolates)\n",
        "    \n",
        "    \n",
        "    grad_x_hat = grad(\n",
        "            outputs=d_interpolates.sum(), inputs=interpolates, create_graph=True)[0]\n",
        "    grad_penalty = ((grad_x_hat.view(grad_x_hat.size(0), -1)\n",
        "                      .norm(2, dim=1) - 1)**2).mean()\n",
        "    grad_penalty = 10 * grad_penalty\n",
        "    \n",
        "    return grad_penalty"
      ],
      "metadata": {
        "id": "HpT5gBx3CyAC"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Try FID LOSS"
      ],
      "metadata": {
        "id": "4STG2MN5C4j2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import cupy as np\n",
        "from torchvision.models.feature_extraction import create_feature_extractor\n",
        "from torchvision.models.feature_extraction import get_graph_node_names\n",
        "\n",
        "import scipy.linalg as linalg\n",
        "\n",
        "\n",
        "class FID_Loss(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.model = t.hub.load('pytorch/vision:v0.10.0', 'inception_v3', pretrained=True)\n",
        "\n",
        "    self.model.eval()\n",
        "\n",
        "    self.model = create_feature_extractor(self.model, {'avgpool': 'feat'})\n",
        "\n",
        "\n",
        "  def sqrtm(self, a):\n",
        "    # Computing diagonalization\n",
        "        evalues, evectors = np.linalg.eigh(a)\n",
        "        # Ensuring square root matrix exists\n",
        "        #assert (evalues >= 0).all()\n",
        "        covmean = evectors @ np.diag(np.sqrt(evalues)) @ np.linalg.inv(evectors)\n",
        "        return covmean\n",
        "\n",
        "  # calculate frechet inception distance\n",
        "  # A faster FID calculation modified from https://github.com/mseitzer/pytorch-fid/blob/master/src/pytorch_fid/fid_score.py\n",
        "  def calculate_frechet_distance(self, ac1, ac2, eps=1e-6):\n",
        "      \"\"\"Numpy implementation of the Frechet Distance.\n",
        "      The Frechet distance between two multivariate Gaussians X_1 ~ N(mu_1, C_1)\n",
        "      and X_2 ~ N(mu_2, C_2) is\n",
        "              d^2 = ||mu_1 - mu_2||^2 + Tr(C_1 + C_2 - 2*sqrt(C_1*C_2)).\n",
        "      Stable version by Dougal J. Sutherland.\n",
        "      Params:\n",
        "      -- mu1   : Numpy array containing the activations of a layer of the\n",
        "                inception net (like returned by the function 'get_predictions')\n",
        "                for generated samples.\n",
        "      -- mu2   : The sample mean over activations, precalculated on an\n",
        "                representative data set.\n",
        "      -- sigma1: The covariance matrix over activations for generated samples.\n",
        "      -- sigma2: The covariance matrix over activations, precalculated on an\n",
        "                representative data set.\n",
        "      Returns:\n",
        "      --   : The Frechet Distance.\n",
        "      \"\"\"\n",
        "      \n",
        "      print(\"ac1: \", ac1.shape)\n",
        "\n",
        "      with np.cuda.Device(0):\n",
        "      \n",
        "        ac1 = np.array(ac1.cpu().detach().numpy())\n",
        "        ac2 = np.array(ac2.cpu().detach().numpy())\n",
        "\n",
        "\n",
        "        mu1 = np.mean(ac1)\n",
        "        sigma1 = np.cov(ac1)\n",
        "        \n",
        "        mu2 = np.mean(ac2)\n",
        "        sigma2 = np.cov(ac2)\n",
        "\n",
        "        mu1 = np.atleast_1d(mu1)\n",
        "        mu2 = np.atleast_1d(mu2)\n",
        "\n",
        "        sigma1 = np.atleast_2d(sigma1)\n",
        "        sigma2 = np.atleast_2d(sigma2)\n",
        "\n",
        "        assert mu1.shape == mu2.shape, \\\n",
        "            'Training and test mean vectors have different lengths'\n",
        "        assert sigma1.shape == sigma2.shape, \\\n",
        "            'Training and test covariances have different dimensions'\n",
        "\n",
        "        diff = mu1 - mu2\n",
        "\n",
        "        # Product might be almost singular\n",
        "        print(\"sigma1 shape: \", sigma1.shape)\n",
        "        print(\"sigma2 shape: \", sigma2.shape)\n",
        "\n",
        "\n",
        "        # Computing diagonalization\n",
        "        covmean = self.sqrtm(sigma1.T@sigma2)\n",
        "        #covmean, _ = linalg.sqrtm(sigma1@sigma2.T, disp=False)\n",
        "        print(covmean.shape)\n",
        "        if not np.isfinite(covmean).all():\n",
        "            msg = ('fid calculation produces singular product; '\n",
        "                  'adding %s to diagonal of cov estimates') % eps\n",
        "            print(msg)\n",
        "            offset = np.eye(sigma1.shape[0]) * eps\n",
        "            covmean = self.sqrtm((sigma1 + offset).dot(sigma2 + offset))\n",
        "\n",
        "        # Numerical error might give slight imaginary component\n",
        "        if np.iscomplexobj(covmean):\n",
        "            if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n",
        "                m = np.max(np.abs(covmean.imag))\n",
        "                raise ValueError('Imaginary component {}'.format(m))\n",
        "            covmean = covmean.real\n",
        "\n",
        "        tr_covmean = np.trace(covmean)\n",
        "\n",
        "        return  100 * (diff.dot(diff) + np.trace(sigma1)\n",
        "                + np.trace(sigma2) - 2 * tr_covmean)\n",
        "      \n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, fake, real):\n",
        "\n",
        "    with t.no_grad():\n",
        "    \n",
        "      upsamle_layer = t.nn.UpsamplingBilinear2d(size=299)\n",
        "\n",
        "      transform = transforms.Compose([\n",
        "                                    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "      ])\n",
        "\n",
        "      fake = upsamle_layer(fake)\n",
        "      fake = transform(fake)\n",
        "\n",
        "      fake_feature = self.model(fake)['feat']\n",
        "      fake_feature = fake_feature.reshape((fake_feature.shape[0], 2048))\n",
        "\n",
        "      real = upsamle_layer(real)\n",
        "      real = transform(real)\n",
        "\n",
        "      real_feature = self.model(real)['feat']\n",
        "      real_feature = real_feature.reshape((real_feature.shape[0], 2048))\n",
        "      print(\"real_feature: \", real_feature.shape)\n",
        "      FID_score = self.calculate_frechet_distance(real_feature, fake_feature)\n",
        "      FID_score = t.tensor(FID_score)\n",
        "    return FID_score\n",
        "\n",
        "\n",
        "FID_loss = FID_Loss().to(device)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Run1jf4C6QM",
        "outputId": "7fdaf1e4-3ca0-47ab-a49c-0016467c1267"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/feature_extraction.py:175: UserWarning: NOTE: The nodes obtained by tracing the model in eval mode are a subsequence of those obtained in train mode. When choosing nodes for feature extraction, you may need to specify output nodes for train and eval mode separately.\n",
            "  warnings.warn(msg + suggestion_msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# \"Testing FID Loss\"\n",
        "# fake = t.randn((64, 3, 16, 16)).to(device)\n",
        "# real = t.randn((64, 3, 16, 16)).to(device)\n",
        "\n",
        "# #same = FID_loss(fake, fake)\n",
        "\n",
        "# dif = FID_loss(fake, real)\n",
        "\n",
        "# dif = dif.reshape((1,1))\n",
        "\n",
        "# dif = t.asarray(dif)\n",
        "\n",
        "# a = nn.Sequential(\n",
        "#     nn.Linear(1, 10),\n",
        "#     nn.Linear(10, 1)\n",
        "# )\n",
        "\n",
        "# a_opt = t.optim.Adam(a.parameters(), lr = 0.001)\n",
        "\n",
        "# loss = a(dif.float())\n",
        "\n",
        "# loss.backward()\n",
        "\n",
        "# print(a.state_dict())\n",
        "\n",
        "\n",
        "# #print(\"same: \", same)\n",
        "# print(\"dif\", dif)\n"
      ],
      "metadata": {
        "id": "Fw0SL-95z554"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initializing the generator, discriminator, optimizer, labels, and loss."
      ],
      "metadata": {
        "id": "V0RdF6DhC8pD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generator = G().to(device)\n",
        "discriminator = D().to(device)\n",
        "\n",
        "G_optimizer = t.optim.Adam(generator.parameters(), lr = 0.00001)\n",
        "D_optimizer = t.optim.Adam(discriminator.parameters(), lr = 0.00001)\n",
        "\n",
        "label_real = t.ones((batch_size, 1)).to(device)\n",
        "label_fake = t.zeros((batch_size, 1)).to(device)"
      ],
      "metadata": {
        "id": "rpHbmg47DI1d"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train decider"
      ],
      "metadata": {
        "id": "omNWHyD1DKxc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(iteration):\n",
        "\n",
        "    p = tqdm(range(total_data_len // batch_size + 1)) # This is a progress bar run on each epoch\n",
        "    \n",
        "    datasets_batched = DataLoader(dataset, batch_size = batch_size)\n",
        "    \n",
        "    loader = iter(datasets_batched)\n",
        "\n",
        "    for j in p:\n",
        "        \n",
        "\n",
        "        batchNum = str(i+1)\n",
        "        \n",
        "        #Training the D\n",
        "        #real data\n",
        "        real = next(loader).to(device)\n",
        "\n",
        "        #current_batch size (the last batch is different than others)\n",
        "        current_batch_size, c, h, w = real.shape\n",
        "        #labels\n",
        "        label_real = 0.1 * t.randint(7,10,(current_batch_size,1)).type(t.half)\n",
        "        label_fake = 0.1 * t.randint(0,3,(current_batch_size,1)).type(t.half)\n",
        "\n",
        "        real_score = discriminator(real)\n",
        "        d_real_loss = -t.mean(real_score)\n",
        "        \n",
        "        input_noise = t.normal(0, 1, size = (current_batch_size, 1, 1, laten_space)).to(device)\n",
        "        fake = generator(input_noise, current_batch_size)\n",
        "        fake_score = discriminator(fake)\n",
        "\n",
        "        d_fake_loss = t.mean(fake_score)\n",
        "\n",
        "        gradient_penalty = compute_gradient_penalty(discriminator, real, fake, current_batch_size)\n",
        "        \n",
        "        total_loss = -FID_loss(real, fake)\n",
        "\n",
        "        generator.zero_grad()\n",
        "        discriminator.zero_grad()\n",
        "        total_loss.backward()\n",
        "        D_optimizer.step()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        if j % critic == 0:\n",
        "          input_noise = t.normal(0, 1, size = (current_batch_size, 1, 1, laten_space)).to(device)\n",
        "          fake = generator(input_noise, current_batch_size)\n",
        "          fake_score = discriminator(fake)\n",
        "          g_fake_loss = -t.mean(fake_score)\n",
        "          discriminator.zero_grad()\n",
        "          generator.zero_grad()\n",
        "          g_fake_loss.backward()\n",
        "          G_optimizer.step()\n",
        "\n",
        "          \n",
        "\n",
        "        \n",
        "        mse = \"Epoch: \" + batchNum\n",
        "        \n",
        "        p.set_description(mse)\n",
        "            \n",
        "        p.set_postfix(G_loss = g_fake_loss.item(), D_total_loss = total_loss.item(), real_score = real_score.mean().item(), fake_score = fake_score.mean().item())\n",
        "        \n",
        "    if i == 0:\n",
        "        showOneImge(real[0], 99999, True)   \n",
        "    \n",
        "    if i % 2 == 0:\n",
        "      \n",
        "      showOneImge(real[0], 99999, False)\n",
        "      showOneImge(fake[0], i, False)\n",
        "      print(\"epoch = \", i + 1)  \n",
        "      print('Epoch [{}/{}], Step [{}/{}], d_loss: {:.4f}, g_loss: {:.4f}, D(x): {:.2f}, D(G(z)): {:.2f}'.format(i, iteration, j+1, total_data_len // batch_size + 1, total_loss.item(), g_fake_loss.item(), real_score.mean().item(), fake_score.mean().item()))      \n",
        "      \n",
        "\n",
        "    if i % 50 == 0:\n",
        "      t.save(generator.state_dict(), f'{log_folder}/checkpoint/{str(i + 1).zfill(6)}_g.model')\n",
        "      t.save(discriminator.state_dict(), f'{log_folder}/checkpoint/{str(i + 1).zfill(6)}_d.model')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 677
        },
        "id": "XoEqNnN3DLn3",
        "outputId": "ea40e8a7-0fd8-4c2f-c907-69b32a7fc04a"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/438 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "real_feature:  torch.Size([64, 2048])\n",
            "ac1:  torch.Size([64, 2048])\n",
            "sigma1 shape:  (64, 64)\n",
            "sigma2 shape:  (64, 64)\n",
            "(64, 64)\n",
            "fid calculation produces singular product; adding 1e-06 to diagonal of cov estimates\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-82f94c22397b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mtotal_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mD_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
            "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Show Result"
      ],
      "metadata": {
        "id": "XHL0KU8LDknJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Generated\n",
        "generator.eval()\n",
        "\n",
        "with t.no_grad():\n",
        "  input_noise = t.normal(0, 1, size = (eval_size, 1, 1, laten_space)).to(device)\n",
        "  generated = generator(input_noise, eval_size)\n",
        "  showMoreImages(generated, eval_size)\n",
        "\n",
        "#Real\n",
        "datasets_batched = DataLoader(dataset, batch_size = eval_size)\n",
        "loader = iter(datasets_batched)\n",
        "real = next(loader)\n",
        "showMoreImages(real, eval_size)"
      ],
      "metadata": {
        "id": "xHULWTlKDnNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### remove log file when necessarry"
      ],
      "metadata": {
        "id": "vL3VVCKpDqr8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !rm -rf /content/log/checkpoint\n",
        "\n",
        "# !mkdir /content/log/checkpoint\n",
        "\n",
        "# !rm -rf /content/log/sample\n",
        "# !mkdir /content/log/sample\n",
        "\n",
        "# from google.colab import files\n",
        "# files.download('/content/log') "
      ],
      "metadata": {
        "id": "qlMCFi_IDtmr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}